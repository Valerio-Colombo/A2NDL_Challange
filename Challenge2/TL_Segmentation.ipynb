{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WfmQXOmnUsrk",
    "outputId": "86bf8db5-9841-4ef3-de68-7f63ec4f55ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ]
    }
   ],
   "source": [
    "google_colab = True\n",
    "if google_colab:\n",
    "  %tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIrHHSZ2U2y7"
   },
   "outputs": [],
   "source": [
    "if google_colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7GyHn20RVIbU"
   },
   "outputs": [],
   "source": [
    "if google_colab:\n",
    "  !unzip ann-and-dl-image-segmentation.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-6q_K3IKCVx"
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oZGJMDuxKCV_",
    "outputId": "7826e45d-09c5-4e8a-8884-fa4019f6059e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 8435\n",
    "tf.random.set_seed(SEED)  \n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Set GPU memory growth\n",
    "# Allows to only as much GPU memory as needed\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2LW8-NMKCWI"
   },
   "source": [
    "# Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qE43FprKCWN"
   },
   "outputs": [],
   "source": [
    "# ImageDataGenerator\n",
    "# ------------------\n",
    "\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "apply_data_augmentation = True\n",
    "\n",
    "# Create training ImageDataGenerator object\n",
    "# We need two different generators for images and corresponding masks\n",
    "if apply_data_augmentation:\n",
    "    train_img_data_gen = ImageDataGenerator(rotation_range=10,\n",
    "                                            width_shift_range=10,\n",
    "                                            height_shift_range=10,\n",
    "                                            zoom_range=0.3,\n",
    "                                            horizontal_flip=True,\n",
    "                                            vertical_flip=True,\n",
    "                                            fill_mode='constant',\n",
    "                                            cval=0,\n",
    "                                            validation_split=0.2,\n",
    "                                            preprocessing_function=preprocess_input)\n",
    "    train_mask_data_gen = ImageDataGenerator(rotation_range=10,\n",
    "                                             width_shift_range=10,\n",
    "                                             height_shift_range=10,\n",
    "                                             zoom_range=0.3,\n",
    "                                             horizontal_flip=True,\n",
    "                                             vertical_flip=True,\n",
    "                                             fill_mode='constant',\n",
    "                                             cval=0,\n",
    "                                             validation_split=0.2,\n",
    "                                             rescale=1./255)\n",
    "else:\n",
    "    train_img_data_gen = ImageDataGenerator(rescale=1./255,\n",
    "                                            validation_split=0.2)\n",
    "    train_mask_data_gen = ImageDataGenerator(rescale=1./255,\n",
    "                                             validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "jWZsKMvMKCWW",
    "outputId": "1983ec6a-7f4c-4cbd-8804-0299a413d3b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6118 images belonging to 1 classes.\n",
      "Found 6118 images belonging to 1 classes.\n",
      "Found 1529 images belonging to 1 classes.\n",
      "Found 1529 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create generators to read images from dataset directory\n",
    "# -------------------------------------------------------\n",
    "dataset_dir = os.path.join(cwd, 'Segmentation_Dataset')\n",
    "\n",
    "# Batch size\n",
    "bs = 32\n",
    "\n",
    "# img shape\n",
    "img_h = 256\n",
    "img_w = 256\n",
    "\n",
    "num_classes=2\n",
    "\n",
    "training_dir = os.path.join(dataset_dir, 'training')\n",
    "# Training\n",
    "train_img_gen = train_img_data_gen.flow_from_directory(os.path.join(training_dir, 'images'),\n",
    "                                                       target_size=(img_h, img_w),\n",
    "                                                       batch_size=bs, \n",
    "                                                       class_mode=None,\n",
    "                                                       shuffle=True,\n",
    "                                                       subset='training',\n",
    "                                                       interpolation='bilinear',\n",
    "                                                       color_mode='rgb',\n",
    "                                                       seed=SEED)  \n",
    "train_mask_gen = train_mask_data_gen.flow_from_directory(os.path.join(training_dir, 'masks'),\n",
    "                                                         target_size=(img_h, img_w),\n",
    "                                                         batch_size=bs,\n",
    "                                                         class_mode=None,\n",
    "                                                         shuffle=True,\n",
    "                                                         subset='training',\n",
    "                                                         interpolation='bilinear',\n",
    "                                                         color_mode='grayscale',\n",
    "                                                         seed=SEED)\n",
    "train_gen = zip(train_img_gen, train_mask_gen)\n",
    "\n",
    "# Validation\n",
    "valid_img_gen = train_mask_data_gen.flow_from_directory(os.path.join(training_dir, 'images'),\n",
    "                                                       target_size=(img_h, img_w),\n",
    "                                                       batch_size=bs, \n",
    "                                                       class_mode=None,\n",
    "                                                       shuffle=False,\n",
    "                                                       subset='validation',\n",
    "                                                       interpolation='bilinear',\n",
    "                                                       color_mode='rgb',\n",
    "                                                       seed=SEED)\n",
    "valid_mask_gen = train_mask_data_gen.flow_from_directory(os.path.join(training_dir, 'masks'),\n",
    "                                                         target_size=(img_h, img_w),\n",
    "                                                         batch_size=bs, \n",
    "                                                         class_mode=None,\n",
    "                                                         shuffle=False,\n",
    "                                                         subset='validation',\n",
    "                                                         interpolation='bilinear',\n",
    "                                                         color_mode='grayscale',\n",
    "                                                         seed=SEED)\n",
    "valid_gen = zip(valid_img_gen, valid_mask_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oFD-5r5JKCWc"
   },
   "outputs": [],
   "source": [
    "# Create Dataset objects\n",
    "# ----------------------\n",
    "\n",
    "# Training\n",
    "# --------\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 1]))\n",
    "\n",
    "def prepare_target(x_, y_):\n",
    "    y_ = tf.cast(tf.expand_dims(y_[..., 0], -1), tf.int32)\n",
    "    return x_, tf.where(y_ > 0, y_ - 1, y_ + 1)\n",
    "\n",
    "train_dataset = train_dataset.map(prepare_target)\n",
    "\n",
    "# Repeat\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "# Validation\n",
    "# ----------\n",
    "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 1]))\n",
    "valid_dataset = valid_dataset.map(prepare_target)\n",
    "\n",
    "# Repeat\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jF_158pGKCW0"
   },
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWE6AHNQL9aD"
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "WTNlwR72MEO6",
    "outputId": "97aaccf8-27ec-43f1-83ec-8a759ca9d0e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tensorflow-2.0.0/python3.6/keras_applications/mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=[256, 256, 3], include_top=False)\n",
    "\n",
    "# Use the activations of these layers\n",
    "layer_names = [\n",
    "    'block_1_expand_relu',  \n",
    "    'block_3_expand_relu',  \n",
    "    'block_6_expand_relu',   \n",
    "    'block_13_expand_relu',  \n",
    "    'block_16_project',     \n",
    "]\n",
    "layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "# Create the feature extraction model\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "\n",
    "down_stack.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9dn2FTaWMjME"
   },
   "outputs": [],
   "source": [
    "def upsample(filters, size, apply_dropout=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "  result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "      result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result\n",
    "\n",
    "up_stack = [\n",
    "    upsample(512, 3), \n",
    "    upsample(256, 3), \n",
    "    upsample(128, 3), \n",
    "    upsample(64, 3),  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-oMWvr4PM36t"
   },
   "outputs": [],
   "source": [
    "def unet_model(output_channels):\n",
    "\n",
    "  # This is the last layer of the model\n",
    "  last = tf.keras.layers.Conv2DTranspose(\n",
    "      output_channels, 3, strides=2,\n",
    "      padding='same', activation='softmax') \n",
    "\n",
    "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = down_stack(x)\n",
    "  x = skips[-1]\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    x = concat([x, skip])\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LCtf9XhvKCXL"
   },
   "source": [
    "## Prepare the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "qZPy3DnEKCXO",
    "outputId": "609c385b-3bb9-4919-f6e3-0eefc6d34b91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Model)                   [(None, 128, 128, 96 1841984     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 16, 16, 512)  1476608     model[1][4]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 1088) 0           sequential[0][0]                 \n",
      "                                                                 model[1][3]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 32, 32, 256)  2507776     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 448)  0           sequential_1[0][0]               \n",
      "                                                                 model[1][2]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 64, 64, 128)  516608      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 272)  0           sequential_2[0][0]               \n",
      "                                                                 model[1][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 128, 128, 64) 156928      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 160 0           sequential_3[0][0]               \n",
      "                                                                 model[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 256, 256, 2)  2882        concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,502,786\n",
      "Trainable params: 4,658,882\n",
      "Non-trainable params: 1,843,904\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Optimization params\n",
    "# -------------------\n",
    "\n",
    "# Loss\n",
    "# Sparse Categorical Crossentropy to use integers (mask) instead of one-hot encoded labels\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False) \n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# -------------------\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "def my_IoU(y_true, y_pred):\n",
    "    # from probability to predicted class {0, 1}\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32) # when using sigmoid. Use argmax for softmax\n",
    "\n",
    "    # A and B\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    # A or B\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "    # IoU\n",
    "    return intersection / union\n",
    "\n",
    "metrics = [['accuracy', my_IoU]]\n",
    "# ------------------\n",
    "\n",
    "# Compile Model\n",
    "model = unet_model(OUTPUT_CHANNELS)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "              metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2HKbVm3WKCXU"
   },
   "source": [
    "## Training with callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "colab_type": "code",
    "id": "8eRiB20tKCXW",
    "outputId": "b813d057-f9bd-454c-a7e3-bbbb73c16b51",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 192 steps, validate for 48 steps\n",
      "Epoch 1/30\n",
      "192/192 [==============================] - 155s 805ms/step - loss: 0.3583 - accuracy: 0.8397 - val_loss: 0.4600 - val_accuracy: 0.7812\n",
      "Epoch 2/30\n",
      "192/192 [==============================] - 147s 767ms/step - loss: 0.3151 - accuracy: 0.8625 - val_loss: 0.4369 - val_accuracy: 0.7899\n",
      "Epoch 3/30\n",
      "192/192 [==============================] - 147s 764ms/step - loss: 0.3007 - accuracy: 0.8699 - val_loss: 0.3820 - val_accuracy: 0.8278\n",
      "Epoch 4/30\n",
      "192/192 [==============================] - 147s 766ms/step - loss: 0.2921 - accuracy: 0.8743 - val_loss: 0.3831 - val_accuracy: 0.8261\n",
      "Epoch 5/30\n",
      "192/192 [==============================] - 147s 765ms/step - loss: 0.2869 - accuracy: 0.8770 - val_loss: 0.4546 - val_accuracy: 0.7894\n",
      "Epoch 6/30\n",
      "192/192 [==============================] - 147s 765ms/step - loss: 0.2810 - accuracy: 0.8800 - val_loss: 0.4336 - val_accuracy: 0.8042\n",
      "Epoch 7/30\n",
      "192/192 [==============================] - 147s 765ms/step - loss: 0.2754 - accuracy: 0.8827 - val_loss: 0.4407 - val_accuracy: 0.7948\n",
      "Epoch 8/30\n",
      "192/192 [==============================] - 147s 765ms/step - loss: 0.2730 - accuracy: 0.8841 - val_loss: 0.4677 - val_accuracy: 0.7816\n",
      "Epoch 9/30\n",
      "192/192 [==============================] - 147s 768ms/step - loss: 0.2732 - accuracy: 0.8839 - val_loss: 0.3991 - val_accuracy: 0.8177\n",
      "Epoch 10/30\n",
      "192/192 [==============================] - 148s 771ms/step - loss: 0.2668 - accuracy: 0.8869 - val_loss: 0.4373 - val_accuracy: 0.8000\n",
      "Epoch 11/30\n",
      "192/192 [==============================] - 148s 770ms/step - loss: 0.2669 - accuracy: 0.8870 - val_loss: 0.3636 - val_accuracy: 0.8411\n",
      "Epoch 12/30\n",
      "192/192 [==============================] - 148s 769ms/step - loss: 0.2617 - accuracy: 0.8893 - val_loss: 0.3813 - val_accuracy: 0.8318\n",
      "Epoch 13/30\n",
      "192/192 [==============================] - 151s 785ms/step - loss: 0.2626 - accuracy: 0.8893 - val_loss: 0.5770 - val_accuracy: 0.7398\n",
      "Epoch 14/30\n",
      "192/192 [==============================] - 149s 774ms/step - loss: 0.2595 - accuracy: 0.8903 - val_loss: 0.4916 - val_accuracy: 0.7693\n",
      "Epoch 15/30\n",
      "192/192 [==============================] - 149s 777ms/step - loss: 0.2584 - accuracy: 0.8908 - val_loss: 0.5470 - val_accuracy: 0.7519\n",
      "Epoch 16/30\n",
      "192/192 [==============================] - 149s 775ms/step - loss: 0.2563 - accuracy: 0.8920 - val_loss: 0.4747 - val_accuracy: 0.7843\n",
      "Epoch 17/30\n",
      "192/192 [==============================] - 148s 773ms/step - loss: 0.2545 - accuracy: 0.8926 - val_loss: 0.5183 - val_accuracy: 0.7701\n",
      "Epoch 18/30\n",
      "192/192 [==============================] - 148s 768ms/step - loss: 0.2557 - accuracy: 0.8927 - val_loss: 0.4762 - val_accuracy: 0.7943\n",
      "Epoch 19/30\n",
      "192/192 [==============================] - 147s 768ms/step - loss: 0.2536 - accuracy: 0.8931 - val_loss: 0.4216 - val_accuracy: 0.8150\n",
      "Epoch 20/30\n",
      "192/192 [==============================] - 148s 772ms/step - loss: 0.2519 - accuracy: 0.8940 - val_loss: 0.3986 - val_accuracy: 0.8233\n",
      "Epoch 21/30\n",
      "192/192 [==============================] - 149s 777ms/step - loss: 0.2513 - accuracy: 0.8947 - val_loss: 0.5069 - val_accuracy: 0.7743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c470b7f98>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "exps_dir = os.path.join(cwd, 'segmentation_experiments')\n",
    "if not os.path.exists(exps_dir):\n",
    "    os.makedirs(exps_dir)\n",
    "\n",
    "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "model_name = 'CNN'\n",
    "\n",
    "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "    \n",
    "callbacks = []\n",
    "\n",
    "# Model checkpoint\n",
    "# ----------------\n",
    "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
    "                                                   save_weights_only=True) \n",
    "callbacks.append(ckpt_callback)\n",
    "\n",
    "# Visualize Learning on Tensorboard\n",
    "# ---------------------------------\n",
    "tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "if not os.path.exists(tb_dir):\n",
    "    os.makedirs(tb_dir)\n",
    "    \n",
    "# By default shows losses and metrics for both training and validation\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "                                             profile_batch=0,\n",
    "                                             histogram_freq=0) \n",
    "callbacks.append(tb_callback)\n",
    "\n",
    "# Early Stopping\n",
    "# --------------\n",
    "early_stop = True\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "\n",
    "model.fit(x=train_dataset,\n",
    "          epochs=30,\n",
    "          steps_per_epoch=len(train_img_gen),\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=len(valid_img_gen), \n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7pKaUzDKCXk"
   },
   "source": [
    "## Compute prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3B_qKpRe91a"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(csv_fname, 'w') as f:\n",
    "\n",
    "      f.write('ImageId,EncodedPixels,Width,Height\\n')\n",
    "\n",
    "      for key, value in results.items():\n",
    "          f.write(key + ',' + str(value) + ',' + '256' + ',' + '256' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azJB0sa6_wpO"
   },
   "outputs": [],
   "source": [
    "def rle_encode(img):\n",
    "      # Flatten column-wise\n",
    "      pixels = img.T.flatten()\n",
    "      pixels = np.concatenate([[0], pixels, [0]])\n",
    "      runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "      runs[1::2] -= runs[::2]\n",
    "      return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktq58wsVKCXm",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import PIL\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Cycle over test images\n",
    "\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "test_img_dir = os.path.join(test_dir, 'images', 'img')\n",
    "\n",
    "img_filenames = next(os.walk(test_img_dir))[2]\n",
    "\n",
    "results={}\n",
    "\n",
    "for img_filename in img_filenames:\n",
    "    \n",
    "    img = Image.open(os.path.join(test_img_dir, img_filename)).convert('RGB')\n",
    "    \n",
    "    img_filename = img_filename[:-4]\n",
    "    img_arr = np.expand_dims(np.array(img), 0)\n",
    "    img_arr = img_arr / 255.0\n",
    "    out_softmax = model.predict(x=img_arr)\n",
    "\n",
    "    # Get predicted class as the index corresponding to the maximum value in the vector probability\n",
    "    predicted_class = tf.argmax(out_softmax, -1)\n",
    "    predicted_class = predicted_class[0]\n",
    "    prediction_img = np.zeros([256, 256, 1])\n",
    "    prediction_img[np.where(predicted_class == 0)] = 1\n",
    "    prediction_img[np.where(predicted_class == 1)] = 0\n",
    "    results[img_filename] = rle_encode(prediction_img)\n",
    "\n",
    "create_csv(results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of CNN_Segmentation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
