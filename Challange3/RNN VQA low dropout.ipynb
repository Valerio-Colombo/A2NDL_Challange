{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom datetime import datetime\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models, utils","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## System checkup"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the seed for random operations. \n# This let our experiments to be reproducible. \nSEED = 5231\ntf.random.set_seed(SEED)  \n\n# Get current working directory\nkaggle = True\nif kaggle:\n    cwd = os.path.join('../input/ann-and-dl-vqa')\nelse:\n    cwd = os.getcwd()\n\n# Set GPU memory growth \n# Allows to only as much GPU memory as needed\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Batch size\nbs = 64\n\n# Validation size\nval_size = 0.2\n\n# img shape\nimg_w = 480\nimg_h = 320\n\n# question shape\nq_len = 100\n\n# dictionary var\nMAX_NUM_QUESTIONS = 30000\nMAX_NUM_WORDS = 20000\n\n# class shape\nnum_classes=13\nclasses = ['0',     #0\n           '1',     #1\n           '10',    #2\n           '2',     #3\n           '3',     #4\n           '4',     #5\n           '5',     #6\n           '6',     #7\n           '7',     #8\n           '8',     #9\n           '9',     #10\n           'no',    #11\n           'yes']   #12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\ndataset_dir = os.path.join(cwd, 'dataset_vqa')\n\nquestions = [] # Array containing all training and validation questions\nquestions_test = []  # Array containing all test questions\ntrain_data = [] # Array containing all training {question, image, answer} dict\nvalid_data = [] # Array containing all validation {question, image, answer} dict\ntest_data = [] # Array containing all test {question_id, image_filename, question} dict\n\nwith open(dataset_dir + '/train_data.json', 'r') as f:\n    train_d = json.load(f)[\"questions\"]\n    if(MAX_NUM_QUESTIONS < len(train_d)):\n        train_d = train_d[:MAX_NUM_QUESTIONS]\n    questions = [e['question'] for e in train_d]\n    valid_data = train_d[:int(len(train_d)*val_size)]\n    train_data = train_d[int(len(train_d)*val_size):]\n    print('Train questions: ' + str(len(train_data)))\n    print('Validation questions: ' + str(len(valid_data)))\n\nwith open(dataset_dir + '/test_data.json', 'r') as f:\n    test_data = json.load(f)[\"questions\"]\n    questions_test= [e['question'] for e in test_data]\n    print('Test questions: ' + str(len(test_data)))    \n    \n#print(questions[:10])\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nquestions_all = questions + questions_test\n\n# Create Tokenizer to convert words to integers\nq_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\nq_tokenizer.fit_on_texts(questions_all)\nq_tokenized = q_tokenizer.texts_to_sequences(questions_all)\n\nq_wtoi = q_tokenizer.word_index\nprint('Total question words:', len(q_wtoi))\n\nmax_q_length = max(len(sentence) for sentence in q_tokenized)\nprint('Max question length:', max_q_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pad to max question length\nq_encoder_inputs = pad_sequences(q_tokenized, maxlen=max_q_length)\nprint(\"Question encoder inputs shape:\", q_encoder_inputs.shape)\n\nq_encoder_inputs_train = q_encoder_inputs[int(len(train_d)*val_size):int(len(train_d))]\nq_encoder_inputs_valid = q_encoder_inputs[:int(len(train_d)*val_size)]\nq_encoder_inputs_test = q_encoder_inputs[int(len(train_d)):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Custom data generator\n# data: json file with (question, image_name, answer) tuples\n# batch_size: size of batches\nimport math\nfrom skimage.io import imread\n\nclass VQASequence(utils.Sequence):\n        def __init__(self, data, q, batch_size):\n            self.x = list(zip([e['image_filename'] for e in data], q)) # [image_name, question]\n            self.y = [classes.index(e['answer']) for e in data] # target classes\n            self.batch_size = batch_size\n\n        def __len__(self):\n            return math.ceil(len(self.y) / self.batch_size)\n        \n        def __getitem__(self, idx):\n            batch_x = self.x[idx*self.batch_size : (idx + 1)*self.batch_size]\n            batch_y = self.y[idx*self.batch_size : (idx + 1)*self.batch_size]\n\n            return [np.array([self.__imgtoarray__(e[0]) for e in batch_x]), np.array([e[1] for e in batch_x])], np.array(batch_y)\n        \n        def __imgtoarray__(self, img):\n            if kaggle:\n                im = Image.open('/kaggle/input/ann-and-dl-vqa/dataset_vqa/train/'+img).convert('RGB')\n            else:\n                im = Image.open('dataset_vqa/train/'+img).convert('RGB')\n            np_im = np.array(im)\n            #print(np_im.shape)\n            return np_im/255.0\n        \nclass VQASequenceTest(utils.Sequence):\n        def __init__(self, data, q, batch_size):\n            self.x = list(zip([e['image_filename'] for e in data], q)) # [image_name, question]\n            self.y = [0 for e in data] # target classes\n            self.batch_size = batch_size\n\n        def __len__(self):\n            return math.ceil(len(self.y) / self.batch_size)\n        \n        def __getitem__(self, idx):\n            batch_x = self.x[idx*self.batch_size : (idx + 1)*self.batch_size]\n            batch_y = self.y[idx*self.batch_size : (idx + 1)*self.batch_size]\n\n            return [np.array([self.__imgtoarray__(e[0]) for e in batch_x]), np.array([e[1] for e in batch_x])], np.array(batch_y)\n        \n        def __on_epoch_end__(self):\n            print('end')\n        \n        def __imgtoarray__(self, img):\n            if kaggle:\n                im = Image.open('/kaggle/input/ann-and-dl-vqa/dataset_vqa/test/'+img).convert('RGB')\n            else:\n                im = Image.open('dataset_vqa/test/'+img).convert('RGB')\n            np_im = np.array(im)\n            #print(np_im.shape)\n            return np_im/255.0\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vqa_generator_train = VQASequence(train_data, q_encoder_inputs_train, bs)\nvqa_generator_valid = VQASequence(valid_data, q_encoder_inputs_valid, bs)\nvqa_generator_test = VQASequenceTest(test_data, q_encoder_inputs_test, bs)\n#print(vqa_generator_train.__getitem__(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotImages(question_arr, subset):\n    num = len(question_arr)\n    fig, axes = plt.subplots(num, 1, figsize=(30,30))\n    axes = axes.flatten()\n    for q, ax in zip(question_arr, axes):\n        img = Image.open(dataset_dir + '/' + subset + '/' + q[\"image_filename\"])\n        ax.imshow(img)\n        ax.set_title(q[\"question\"])\n        ax.axis('off')\n  \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotImages(train_data[:2], 'train')\n#plotImages(test_data[:2], 'test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CNN for image encoding\n\n# Load VGG16 Model\nvgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n\nfinetuning = True\n\nif finetuning:\n    freeze_until = 15 # layer from which we want to fine-tune\n    \n    for layer in vgg.layers[:freeze_until]:\n        layer.trainable = False\nelse:\n    vgg.trainable = False\n\n# Image encoding\nimage_model = models.Sequential()\nimage_model.add(vgg)\nimage_model.add(layers.Flatten())\nimage_model.add(layers.Dense(512, activation='relu'))\nimage_model.add(layers.Dropout(rate=0.2, seed=SEED))\n\nimage_input = layers.Input(shape=(img_h, img_w, 3))\n\nencoded_image = image_model(image_input)\n#encoded_image = image_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RNN for question encoding\nquestion_model = models.Sequential()\nquestion_model.add(layers.Embedding(input_dim=len(q_wtoi)+1, output_dim=100, input_length=max_q_length))\nquestion_model.add(layers.LSTM(units=256, return_sequences=True))\nquestion_model.add(layers.Dropout(0.1, seed=SEED))\nquestion_model.add(layers.LSTM(units=256, return_sequences=False))\nquestion_model.add(layers.Dropout(0.1, seed=SEED))\nquestion_model.add(layers.Dense(512, activation='tanh'))\n\nquestion_input = layers.Input(shape=[max_q_length])\nencoded_question = question_model(question_input)\n\n#question_input = layers.Input(shape=[max_q_length])\n#embedded_question = layers.Embedding(input_dim=len(q_wtoi)+1, output_dim=10, input_length=max_q_length)(question_input)\n#encoded_question = layers.LSTM(128)(embedded_question)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine CNN and RNN to create the final model\nmerged = layers.concatenate([encoded_question, encoded_image])\ndense = layers.Dense(512, activation='relu')(merged)\noutput = layers.Dense(num_classes, activation='softmax')(dense)\nvqa_model = models.Model(inputs=[image_input, question_input], outputs=output)\n\nvqa_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cwd = os.getcwd()\n\n# Create a folder which will contain the result of all the run of the network\nexps_dir = os.path.join(cwd, 'classification_experiments')\nif not os.path.exists(exps_dir):\n    os.makedirs(exps_dir)\n\nnow = datetime.now().strftime('%b%d_%H-%M-%S')\n\nmodel_name = 'RNN'\n\n# Create a folder which will contain the result of callbacks of a singular execution\nexp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\nif not os.path.exists(exp_dir):\n    os.makedirs(exp_dir)\n    \ncallbacks = []\n\n# Callback1 - Model checkpoint\nckpt = False\n\nif ckpt:\n    ckpt_dir = os.path.join(exp_dir, 'ckpts')\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n\n    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n                                                       save_weights_only=True) # False to save the model directly\n    callbacks.append(ckpt_callback)\n\n# Callback2 - Early Stopping\nearly_stop = True\n\nif early_stop:\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n    callbacks.append(es_callback)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimization params\nepoch_num = 100\n\n# Loss\nloss = tf.keras.losses.SparseCategoricalCrossentropy()\n\n# Optimazer\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n\n# Validation metrics\nmetrics = ['accuracy']\n\n# Compile Model\nvqa_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = vqa_model.fit_generator(generator = vqa_generator_train,\n                                  callbacks=callbacks,\n                                  epochs=epoch_num,\n                                  steps_per_epoch=vqa_generator_train.__len__(),\n                                  validation_data= vqa_generator_valid,\n                                  validation_steps=vqa_generator_valid.__len__())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label = 'val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.ylim([0.5, 1.75])\nplt.legend(loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\n\ndef create_csv(results, results_dir='./Test_Result'):\n\n    csv_fname = 'results_'\n    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n    \n    exps_dir = os.path.join(cwd, results_dir)\n    if not os.path.exists(exps_dir):\n        os.makedirs(exps_dir)\n\n    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n\n        f.write('Id,Category\\n')\n\n        for key, value in results.items():\n            f.write(key + ',' + str(value) + '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = vqa_model.predict_generator(vqa_generator_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_argmax = []\n\nfor e in prediction:\n    prediction_argmax.append(np.argmax(e))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\n\nresults = {}\n\nwhile(i<len(prediction)):\n    results[str(i)] = prediction_argmax[i]\n    i+=1\n\ncreate_csv(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"TensorFlowEnv","language":"python","name":"tensorflowenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}