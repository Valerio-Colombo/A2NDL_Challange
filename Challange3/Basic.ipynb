{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System checkup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 5231\n",
    "tf.random.set_seed(SEED)  \n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Set GPU memory growth \n",
    "# Allows to only as much GPU memory as needed\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "bs = 6\n",
    "\n",
    "# Validation size\n",
    "val_size = 0.2\n",
    "\n",
    "# img shape\n",
    "img_w = 480\n",
    "img_h = 320\n",
    "\n",
    "# question shape\n",
    "q_len = 100\n",
    "\n",
    "# dictionary var\n",
    "MAX_NUM_SENTENCES = 40000\n",
    "MAX_NUM_WORDS = 20000\n",
    "\n",
    "# class shape\n",
    "num_classes=13\n",
    "classes = ['0',     #0\n",
    "           '1',     #1\n",
    "           '10',    #2\n",
    "           '2',     #3\n",
    "           '3',     #4\n",
    "           '4',     #5\n",
    "           '5',     #6\n",
    "           '6',     #7\n",
    "           '7',     #8\n",
    "           '8',     #9\n",
    "           '9',     #10\n",
    "           'no',    #11\n",
    "           'yes']   #12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset_dir = os.path.join(cwd, 'dataset_vqa')\n",
    "\n",
    "questions = [] # # Array containing all training and validation {question, image, answer} dict\n",
    "train_data = [] # Array containing all training {question, image, answer} dict\n",
    "valid_data = [] # Array containing all validation {question, image, answer} dict\n",
    "test_data = [] # Array containing all test {question, image, answer} dict\n",
    "\n",
    "with open(dataset_dir + '/train_data.json', 'r') as f:\n",
    "    train_d = json.load(f)[\"questions\"]\n",
    "    questions = [e['question'] for e in train_d]\n",
    "    valid_data = train_d[:int(len(train_d)*val_size)]\n",
    "    train_data = train_d[int(len(train_d)*val_size):]\n",
    "    print('Train questions: ' + str(len(train_data)))\n",
    "    print('Validation questions: ' + str(len(valid_data)))\n",
    "\n",
    "with open(dataset_dir + '/test_data.json', 'r') as f:\n",
    "    test_data = json.load(f)[\"questions\"]\n",
    "    print('Test questions: ' + str(len(test_data)))    \n",
    "    \n",
    "#print(questions[:10])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Create Tokenizer to convert words to integers\n",
    "q_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "q_tokenizer.fit_on_texts(questions)\n",
    "q_tokenized = q_tokenizer.texts_to_sequences(questions)\n",
    "\n",
    "q_wtoi = q_tokenizer.word_index\n",
    "print('Total question words:', len(q_wtoi))\n",
    "\n",
    "max_q_length = max(len(sentence) for sentence in q_tokenized)\n",
    "print('Max question length:', max_q_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad to max question length\n",
    "q_encoder_inputs = pad_sequences(q_tokenized, maxlen=max_q_length)\n",
    "print(\"Question encoder inputs shape:\", q_encoder_inputs.shape)\n",
    "\n",
    "q_encoder_inputs_train = q_encoder_inputs[int(len(train_d)*val_size):]\n",
    "q_encoder_inputs_valid = q_encoder_inputs[:int(len(train_d)*val_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data generator\n",
    "# data: json file with (question, image_name, answer) tuples\n",
    "# batch_size: size of batches\n",
    "import math\n",
    "from skimage.io import imread\n",
    "\n",
    "class VQASequence(utils.Sequence):\n",
    "        def __init__(self, data, q, batch_size):\n",
    "            self.x = list(zip([e['image_filename'] for e in data], q)) # [image_name, question]\n",
    "            self.y = [classes.index(e['answer']) for e in data] # target classes\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        def __len__(self):\n",
    "            return math.ceil(len(self.y) / self.batch_size)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            batch_x = self.x[idx*self.batch_size : (idx + 1)*self.batch_size]\n",
    "            batch_y = self.y[idx*self.batch_size : (idx + 1)*self.batch_size]\n",
    "\n",
    "            return [np.array([self.__imgtoarray__(e[0]) for e in batch_x]), np.array([e[1] for e in batch_x])], np.array(batch_y)\n",
    "        \n",
    "        def __imgtoarray__(self, img):\n",
    "            im = Image.open('dataset_vqa/train/'+img).convert('RGB')\n",
    "            np_im = np.array(im)\n",
    "            #print(np_im.shape)\n",
    "            return np_im/255.0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_genearator_train = VQASequence(train_data, q_encoder_inputs_train, bs)\n",
    "vqa_genearator_valid = VQASequence(valid_data, q_encoder_inputs_valid, bs)\n",
    "\n",
    "#print(vqa_genearator_train.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(question_arr, subset):\n",
    "    num = len(question_arr)\n",
    "    fig, axes = plt.subplots(num, 1, figsize=(30,30))\n",
    "    axes = axes.flatten()\n",
    "    for q, ax in zip(question_arr, axes):\n",
    "        img = Image.open(dataset_dir + '/' + subset + '/' + q[\"image_filename\"])\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(q[\"question\"])\n",
    "        ax.axis('off')\n",
    "  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotImages(train_data[:2], 'train')\n",
    "#plotImages(test_data[:2], 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN for image encoding\n",
    "\n",
    "# Load VGG16 Model\n",
    "vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
    "\n",
    "finetuning = False\n",
    "\n",
    "if finetuning:\n",
    "    freeze_until = 15 # layer from which we want to fine-tune\n",
    "    \n",
    "    for layer in vgg.layers[:freeze_until]:\n",
    "        layer.trainable = False\n",
    "else:\n",
    "    vgg.trainable = False\n",
    "\n",
    "# Image encoding\n",
    "image_model = models.Sequential()\n",
    "image_model.add(vgg)\n",
    "image_model.add(layers.Flatten())\n",
    "\n",
    "image_input = layers.Input(shape=(img_h, img_w, 3))\n",
    "\n",
    "encoded_image = image_model(image_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN for question encoding\n",
    "\n",
    "question_input = layers.Input(shape=[max_q_length])\n",
    "embedded_question = layers.Embedding(input_dim=len(q_wtoi)+1, output_dim=256, input_length=max_q_length)(question_input)\n",
    "encoded_question = layers.LSTM(256)(embedded_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine CNN and RNN to create the final model\n",
    "merged = layers.concatenate([encoded_question, encoded_image])\n",
    "output = layers.Dense(num_classes, activation='softmax')(merged)\n",
    "vqa_model = models.Model(inputs=[image_input, question_input], outputs=output)\n",
    "\n",
    "vqa_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "# Create a folder which will contain the result of all the run of the network\n",
    "exps_dir = os.path.join(cwd, 'classification_experiments')\n",
    "if not os.path.exists(exps_dir):\n",
    "    os.makedirs(exps_dir)\n",
    "\n",
    "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "model_name = 'CNN'\n",
    "\n",
    "# Create a folder which will contain the result of callbacks of a singular execution\n",
    "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "    \n",
    "callbacks = []\n",
    "\n",
    "# Callback1 - Model checkpoint\n",
    "ckpt = False\n",
    "\n",
    "if ckpt:\n",
    "    ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
    "                                                       save_weights_only=True) # False to save the model directly\n",
    "    callbacks.append(ckpt_callback)\n",
    "\n",
    "# Callback2 - Early Stopping\n",
    "early_stop = True\n",
    "\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    callbacks.append(es_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization params\n",
    "epoch_num = 100\n",
    "\n",
    "# Loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Optimazer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Validation metrics\n",
    "metrics = ['accuracy']\n",
    "\n",
    "# Compile Model\n",
    "vqa_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = vqa_model.fit_generator(generator = vqa_genearator_train,\n",
    "                                  callbacks=callbacks,\n",
    "                                  epochs=epoch_num,\n",
    "                                  steps_per_epoch=vqa_genearator_train.__len__(),\n",
    "                                  validation_data= vqa_genearator_valid,\n",
    "                                  validation_steps=vqa_genearator_valid.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlowEnv",
   "language": "python",
   "name": "tensorflowenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
