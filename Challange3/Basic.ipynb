{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System checkup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 5231\n",
    "tf.random.set_seed(SEED)  \n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Set GPU memory growth \n",
    "# Allows to only as much GPU memory as needed\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "bs = 6\n",
    "\n",
    "# Validation size\n",
    "val_size = 0.2\n",
    "\n",
    "# img shape\n",
    "img_w = 480\n",
    "img_h = 320\n",
    "\n",
    "# question shape\n",
    "q_len = 100\n",
    "\n",
    "# dictionary var\n",
    "MAX_NUM_QUESTIONS = 5000\n",
    "MAX_NUM_WORDS = 20000\n",
    "\n",
    "# class shape\n",
    "num_classes=13\n",
    "classes = ['0',     #0\n",
    "           '1',     #1\n",
    "           '10',    #2\n",
    "           '2',     #3\n",
    "           '3',     #4\n",
    "           '4',     #5\n",
    "           '5',     #6\n",
    "           '6',     #7\n",
    "           '7',     #8\n",
    "           '8',     #9\n",
    "           '9',     #10\n",
    "           'no',    #11\n",
    "           'yes']   #12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train questions: 4000\n",
      "Validation questions: 1000\n",
      "Test questions: 3000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "dataset_dir = os.path.join(cwd, 'dataset_vqa')\n",
    "\n",
    "questions = [] # Array containing all training and validation questions\n",
    "questions_test = []  # Array containing all test questions\n",
    "train_data = [] # Array containing all training {question, image, answer} dict\n",
    "valid_data = [] # Array containing all validation {question, image, answer} dict\n",
    "test_data = [] # Array containing all test {question_id, image_filename, question} dict\n",
    "\n",
    "with open(dataset_dir + '/train_data.json', 'r') as f:\n",
    "    train_d = json.load(f)[\"questions\"]\n",
    "    if(MAX_NUM_QUESTIONS < len(train_d)):\n",
    "        train_d = train_d[:MAX_NUM_QUESTIONS]\n",
    "    questions = [e['question'] for e in train_d]\n",
    "    valid_data = train_d[:int(len(train_d)*val_size)]\n",
    "    train_data = train_d[int(len(train_d)*val_size):]\n",
    "    print('Train questions: ' + str(len(train_data)))\n",
    "    print('Validation questions: ' + str(len(valid_data)))\n",
    "\n",
    "with open(dataset_dir + '/test_data.json', 'r') as f:\n",
    "    test_data = json.load(f)[\"questions\"]\n",
    "    questions_test= [e['question'] for e in test_data]\n",
    "    print('Test questions: ' + str(len(test_data)))    \n",
    "    \n",
    "#print(questions[:10])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total question words: 70\n",
      "Max question length: 39\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "questions_all = questions + questions_test\n",
    "\n",
    "# Create Tokenizer to convert words to integers\n",
    "q_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "q_tokenizer.fit_on_texts(questions_all)\n",
    "q_tokenized = q_tokenizer.texts_to_sequences(questions_all)\n",
    "\n",
    "q_wtoi = q_tokenizer.word_index\n",
    "print('Total question words:', len(q_wtoi))\n",
    "\n",
    "max_q_length = max(len(sentence) for sentence in q_tokenized)\n",
    "print('Max question length:', max_q_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question encoder inputs shape: (8000, 39)\n"
     ]
    }
   ],
   "source": [
    "# Pad to max question length\n",
    "q_encoder_inputs = pad_sequences(q_tokenized, maxlen=max_q_length)\n",
    "print(\"Question encoder inputs shape:\", q_encoder_inputs.shape)\n",
    "\n",
    "q_encoder_inputs_train = q_encoder_inputs[int(len(train_d)*val_size):int(len(train_d))]\n",
    "q_encoder_inputs_valid = q_encoder_inputs[:int(len(train_d)*val_size)]\n",
    "q_encoder_inputs_test = q_encoder_inputs[:int(len(train_d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data generator\n",
    "# data: json file with (question, image_name, answer) tuples\n",
    "# batch_size: size of batches\n",
    "import math\n",
    "from skimage.io import imread\n",
    "\n",
    "class VQASequence(utils.Sequence):\n",
    "        def __init__(self, data, q, batch_size):\n",
    "            self.x = list(zip([e['image_filename'] for e in data], q)) # [image_name, question]\n",
    "            self.y = [classes.index(e['answer']) for e in data] # target classes\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        def __len__(self):\n",
    "            return math.ceil(len(self.y) / self.batch_size)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            batch_x = self.x[idx*self.batch_size : (idx + 1)*self.batch_size]\n",
    "            batch_y = self.y[idx*self.batch_size : (idx + 1)*self.batch_size]\n",
    "\n",
    "            return [np.array([self.__imgtoarray__(e[0]) for e in batch_x]), np.array([e[1] for e in batch_x])], np.array(batch_y)\n",
    "        \n",
    "        def __imgtoarray__(self, img):\n",
    "            im = Image.open('dataset_vqa/train/'+img).convert('RGB')\n",
    "            np_im = np.array(im)\n",
    "            #print(np_im.shape)\n",
    "            return np_im/255.0\n",
    "        \n",
    "class VQASequenceTest(utils.Sequence):\n",
    "        def __init__(self, data, q, batch_size):\n",
    "            self.x = list(zip([e['image_filename'] for e in data], q)) # [image_name, question]\n",
    "            self.y = [0 for e in data] # target classes\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        def __len__(self):\n",
    "            return math.ceil(len(self.y) / self.batch_size)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            batch_x = self.x[idx*self.batch_size : (idx + 1)*self.batch_size]\n",
    "            batch_y = self.y[idx*self.batch_size : (idx + 1)*self.batch_size]\n",
    "\n",
    "            return [np.array([self.__imgtoarray__(e[0]) for e in batch_x]), np.array([e[1] for e in batch_x])], np.array(batch_y)\n",
    "        \n",
    "        def __on_epoch_end__(self):\n",
    "            print('end')\n",
    "        \n",
    "        def __imgtoarray__(self, img):\n",
    "            im = Image.open('dataset_vqa/test/'+img).convert('RGB')\n",
    "            np_im = np.array(im)\n",
    "            #print(np_im.shape)\n",
    "            return np_im/255.0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_generator_train = VQASequence(train_data, q_encoder_inputs_train, bs)\n",
    "vqa_generator_valid = VQASequence(valid_data, q_encoder_inputs_valid, bs)\n",
    "vqa_generator_test = VQASequenceTest(test_data, q_encoder_inputs_test, bs)\n",
    "#print(vqa_generator_train.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(question_arr, subset):\n",
    "    num = len(question_arr)\n",
    "    fig, axes = plt.subplots(num, 1, figsize=(30,30))\n",
    "    axes = axes.flatten()\n",
    "    for q, ax in zip(question_arr, axes):\n",
    "        img = Image.open(dataset_dir + '/' + subset + '/' + q[\"image_filename\"])\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(q[\"question\"])\n",
    "        ax.axis('off')\n",
    "  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotImages(train_data[:2], 'train')\n",
    "#plotImages(test_data[:2], 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN for image encoding\n",
    "\n",
    "# Load VGG16 Model\n",
    "vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
    "\n",
    "finetuning = False\n",
    "\n",
    "if finetuning:\n",
    "    freeze_until = 15 # layer from which we want to fine-tune\n",
    "    \n",
    "    for layer in vgg.layers[:freeze_until]:\n",
    "        layer.trainable = False\n",
    "else:\n",
    "    vgg.trainable = False\n",
    "\n",
    "# Image encoding\n",
    "image_model = models.Sequential()\n",
    "image_model.add(vgg)\n",
    "image_model.add(layers.Flatten())\n",
    "\n",
    "image_input = layers.Input(shape=(img_h, img_w, 3))\n",
    "\n",
    "encoded_image = image_model(image_input)\n",
    "#encoded_image = image_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN for question encoding\n",
    "\n",
    "question_input = layers.Input(shape=[max_q_length])\n",
    "embedded_question = layers.Embedding(input_dim=len(q_wtoi)+1, output_dim=10, input_length=max_q_length)(question_input)\n",
    "encoded_question = layers.LSTM(128)(embedded_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 39)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 39, 10)       710         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 320, 480, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 128)          71168       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 76800)        14714688    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 76928)        0           lstm[0][0]                       \n",
      "                                                                 sequential[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 13)           1000077     concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 15,786,643\n",
      "Trainable params: 1,071,955\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Combine CNN and RNN to create the final model\n",
    "merged = layers.concatenate([encoded_question, encoded_image])\n",
    "output = layers.Dense(num_classes, activation='softmax')(merged)\n",
    "vqa_model = models.Model(inputs=[image_input, question_input], outputs=output)\n",
    "\n",
    "vqa_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "# Create a folder which will contain the result of all the run of the network\n",
    "exps_dir = os.path.join(cwd, 'classification_experiments')\n",
    "if not os.path.exists(exps_dir):\n",
    "    os.makedirs(exps_dir)\n",
    "\n",
    "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "model_name = 'CNN'\n",
    "\n",
    "# Create a folder which will contain the result of callbacks of a singular execution\n",
    "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "    \n",
    "callbacks = []\n",
    "\n",
    "# Callback1 - Model checkpoint\n",
    "ckpt = False\n",
    "\n",
    "if ckpt:\n",
    "    ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
    "                                                       save_weights_only=True) # False to save the model directly\n",
    "    callbacks.append(ckpt_callback)\n",
    "\n",
    "# Callback2 - Early Stopping\n",
    "early_stop = True\n",
    "\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    callbacks.append(es_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization params\n",
    "epoch_num = 100\n",
    "\n",
    "# Loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Optimazer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
    "\n",
    "# Validation metrics\n",
    "metrics = ['accuracy']\n",
    "\n",
    "# Compile Model\n",
    "vqa_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "667/667 [==============================] - 415s 622ms/step - loss: 2.0224 - accuracy: 0.2105 - val_loss: 1.8390 - val_accuracy: 0.2280\n",
      "Epoch 2/100\n",
      "667/667 [==============================] - 420s 630ms/step - loss: 1.4688 - accuracy: 0.4175 - val_loss: 1.6025 - val_accuracy: 0.3190\n",
      "Epoch 3/100\n",
      "667/667 [==============================] - 426s 638ms/step - loss: 1.3004 - accuracy: 0.4733 - val_loss: 1.5971 - val_accuracy: 0.3470\n",
      "Epoch 4/100\n",
      "667/667 [==============================] - 422s 633ms/step - loss: 1.1818 - accuracy: 0.5160 - val_loss: 1.6509 - val_accuracy: 0.3890\n",
      "Epoch 5/100\n",
      "667/667 [==============================] - 413s 619ms/step - loss: 1.1167 - accuracy: 0.5412 - val_loss: 1.6772 - val_accuracy: 0.3620\n"
     ]
    }
   ],
   "source": [
    "history = vqa_model.fit_generator(generator = vqa_generator_train,\n",
    "                                  callbacks=callbacks,\n",
    "                                  epochs=epoch_num,\n",
    "                                  steps_per_epoch=vqa_generator_train.__len__(),\n",
    "                                  validation_data= vqa_generator_valid,\n",
    "                                  validation_steps=vqa_generator_valid.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def create_csv(results, results_dir='./Test_Result'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(key + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = vqa_model.predict_generator(vqa_generator_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_argmax = []\n",
    "\n",
    "for e in prediction:\n",
    "    prediction_argmax.append(np.argmax(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "\n",
    "results = {}\n",
    "\n",
    "while(i<len(prediction)):\n",
    "    results[str(i)] = prediction_argmax[i]\n",
    "    i+=1\n",
    "\n",
    "create_csv(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlowEnv",
   "language": "python",
   "name": "tensorflowenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
